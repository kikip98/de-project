{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20bfec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import psycopg2\n",
    "from pyspark.sql.functions import when,col,udf\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd1a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing PySpark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "# Spark Config\n",
    "conf = SparkConf().setAppName(\"ML_App\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName('ML_App').getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb685e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------+--------------------+------------------+----------------+---------------------+----------+--------------+-----------+--------------------+----------+---------------+----------------------+--------------+------------+------------------+-------------------+---------------------+--------------+\n",
      "|review_id|        airline|overall_rating|        review_title|     review_author|review_sentiment|review_date_published|  aircraft|traveller_type|cabin_flown|               route|date_flown|value_for_money|inflight_entertainment|ground_service|seat_comfort|food_and_beverages|cabin_staff_service|wifi_and_connectivity|recommendation|\n",
      "+---------+---------------+--------------+--------------------+------------------+----------------+---------------------+----------+--------------+-----------+--------------------+----------+---------------+----------------------+--------------+------------+------------------+-------------------+---------------------+--------------+\n",
      "|       51|British Airways|             1|lost patience air...|           M Kumar|        negative|              04-2022|    unkown|      Business|   Business|  London to Aberdeen|   04-2022|              1|                     0|             3|           1|                 1|                  2|                    0|            no|\n",
      "|       52|British Airways|             1|ba intention reso...|     Paul MacInnes|        negative|              04-2022|      A320|          Solo|   Business|Pisa to London He...|   03-2022|              1|                     1|             1|           1|                 1|                  1|                    1|            no|\n",
      "|       53|British Airways|             1|absolutely shocki...|Charmaine Williams|        negative|              04-2022|Boeing 737|        Couple|    Economy|Johannesburg to D...|   04-2022|              1|                     0|             1|           1|                 1|                  1|                    0|            no|\n",
      "|       54|British Airways|             1|really bad experi...|       Rolf Linden|        negative|              04-2022|      A320|        Couple|    Economy|Gothenburg to Man...|   04-2022|              1|                     0|             1|           3|                 1|                  3|                    1|            no|\n",
      "|       55|British Airways|             9|lots space privac...|        Guy Senior|        positive|              04-2022|Boeing 787|          Solo|   Business|   Chicago to London|   03-2022|              4|                     4|             3|           5|                 5|                  5|                    0|           yes|\n",
      "+---------+---------------+--------------+--------------------+------------------+----------------+---------------------+----------+--------------+-----------+--------------------+----------+---------------+----------------------+--------------+------------+------------------+-------------------+---------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect to DB\n",
    "engine = psycopg2.connect(\n",
    "    database=\"kikipng\",\n",
    "    user=\"postgres\",\n",
    "    password=\"qwerty123\",\n",
    "    host=\"kikipng.ckoss6hrgu4d.eu-west-2.rds.amazonaws.com\",\n",
    "    port=5432)\n",
    "query = \"\"\"select * from airline_postgres_schema.skytrax_reviews\"\"\"\n",
    "df_skytrax  = pd.read_sql(query, engine)\n",
    "\n",
    "# Convert pandas datasframe to spark dataframe\n",
    "dfs_skytrax = spark.createDataFrame(df_skytrax)\n",
    "\n",
    "# Drop missing values\n",
    "dfs_skytrax = dfs_skytrax.dropna()\n",
    "dfs_skytrax.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa391b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop review_author column to anonymize data \n",
    "dfs_skytrax = dfs_skytrax.drop(\"review_author\")\n",
    "\n",
    "# Drop review_title, review_date_published as they are not going to be used in the analysis\n",
    "dfs_skytrax = dfs_skytrax.drop(\"review_title\",\"airline\",\"review_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bbecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column that has 1 if the flight is direct and 0 if not\n",
    "direct_flight = when(col(\"route\").contains(\"via\"), 0).otherwise(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476e28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop route column\n",
    "dfs_skytrax = dfs_skytrax.withColumn(\"direct_flight\",direct_flight)\\\n",
    "            .drop(\"route\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67f0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that does one hot encodings\n",
    "def get_one_hot_encodings(dfs, column_name):   \n",
    "    '''\n",
    "    Input: spark dataframe and name of column we want to one-hot-encode\n",
    "    Output: spark dataframe with one-hot-encoding on the column requested\n",
    "    '''\n",
    "    unique_values = dfs.select(column_name)\\\n",
    "                        .distinct()\\\n",
    "                        .rdd\\\n",
    "                        .flatMap(lambda x: x).collect()\n",
    "\n",
    "    # for each of the gathered values create a new column \n",
    "    for unique_value in unique_values:\n",
    "        function = udf(lambda item: \n",
    "                       1 if item == unique_value else 0, \n",
    "                       IntegerType())\n",
    "        new_column_name = column_name + '_' + unique_value.lower().replace(' ','_')\n",
    "        dfs = dfs.withColumn(new_column_name, function(col(column_name)))\n",
    "    dfs = dfs.drop(column_name)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c41bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot-Encode cabin_flown, traveller_type and airline\n",
    "column_names = ['cabin_flown','traveller_type', 'aircraft']\n",
    "for column_name in column_names:\n",
    "    dfs_skytrax = get_one_hot_encodings(dfs_skytrax,column_name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b74a6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that all categorical columns have been converted to numeric assemble all numeric columns\n",
    "numericCols = [item[0] for item in dfs_skytrax.dtypes if item[1]== 'bigint' or item[1]== 'int' or item[1]== 'double' or item[1]== 'float' ]#[1:]\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "label_stringIdx = StringIndexer(inputCol = 'recommendation', outputCol = 'labelIndex')\n",
    "pipeline = Pipeline(stages=[assembler, label_stringIdx])\n",
    "preprocessing_pipeline_model = pipeline.fit(dfs_skytrax)\n",
    "dfs_skytrax = preprocessing_pipeline_model.transform(dfs_skytrax)\n",
    "\n",
    "\n",
    "train, test= dfs_skytrax.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labelIndex')\n",
    "rfModel = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e391311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00001-00445863-aeeb-49ca-9c99-18dd38218288-c000.snappy.parquet\n",
      "_SUCCESS\n",
      "._SUCCESS.crc\n",
      "part-00000-00445863-aeeb-49ca-9c99-18dd38218288-c000.snappy.parquet\n",
      ".part-00001-00445863-aeeb-49ca-9c99-18dd38218288-c000.snappy.parquet.crc\n",
      ".part-00000-00445863-aeeb-49ca-9c99-18dd38218288-c000.snappy.parquet.crc\n",
      "part-00000\n",
      ".part-00000.crc\n",
      "_SUCCESS\n",
      "._SUCCESS.crc\n",
      "part-00001-4173302a-7d20-429f-982b-7ccfb7377054-c000.snappy.parquet\n",
      "_SUCCESS\n",
      "._SUCCESS.crc\n",
      ".part-00000-4173302a-7d20-429f-982b-7ccfb7377054-c000.snappy.parquet.crc\n",
      "part-00000-4173302a-7d20-429f-982b-7ccfb7377054-c000.snappy.parquet\n",
      ".part-00001-4173302a-7d20-429f-982b-7ccfb7377054-c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def uploadDirectory(path):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id='AKIA2X2ER6BK2I4N3NO4',\n",
    "        aws_secret_access_key='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'\n",
    "    )\n",
    "\n",
    "    s3 = session.resource('s3').Bucket('airline-project-kikipng')\n",
    "    bucketname = 'airline-project-kikipng'\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            s3.upload_file(os.path.join(root,file),os.path.join(root,file))\n",
    "\n",
    "\n",
    "rfModel.write().overwrite().save('model')\n",
    "uploadDirectory('model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44dc4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+\n",
      "|labelIndex|rawPrediction|prediction|probability|\n",
      "+----------+-------------+----------+-----------+\n",
      "|       0.0|   [20.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|       0.0|   [20.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|       0.0|   [20.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|       0.0|   [19.0,1.0]|       0.0|[0.95,0.05]|\n",
      "|       1.0|   [11.0,9.0]|       0.0|[0.55,0.45]|\n",
      "|       0.0|   [20.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|       0.0|   [15.0,5.0]|       0.0|[0.75,0.25]|\n",
      "|       0.0|   [20.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|       0.0|   [16.0,4.0]|       0.0|  [0.8,0.2]|\n",
      "|       1.0|   [1.0,19.0]|       1.0|[0.05,0.95]|\n",
      "+----------+-------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rfModel.transform(test)\n",
    "predictions.select('labelIndex', 'rawPrediction', 'prediction', 'probability').show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b44b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8862745098039215\n",
      "Test Error = 0.11372549019607847\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1823ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "preds_and_labels = predictions.select(['prediction','labelIndex']).withColumn('labelIndex', F.col('labelIndex').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','labelIndex'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
